# ðŸ”¬ Innovation Deep Dive: Logit Bias Calibration (LBC)

## Executive Summary
In the domain of offroad semantic segmentation, class imbalance is a first-order problem. Rare yet critical categories (Rocks, Water, Dynamic Obstacles) are consistently suppressed by the background classes. This document details our **Logit Bias Calibration (LBC)** approachâ€”a post-inference optimization layer that boosts Mean IoU (mIoU) without retraining.

## 1. Mathematical Foundation
Standard inference applies a softmax over the model's logits $z_k$:
$$P(k) = \frac{e^{z_k}}{\sum_{j} e^{z_j}}$$

Our LBC method introduces a learnable bias vector $b \in \mathbb{R}^K$:
$$P_{calibrated}(k) = \frac{e^{z_k + b_k}}{\sum_{j} e^{z_j + b_j}}$$

## 2. Multi-Objective Optimization
Instead of tuning for a single metric, we employ a grid search combined with SciPy's `minimize` function to find the optimal vector $b$ that maximizes the **minimum per-class IoU**.
- **Objective Function**: $J(b) = -\min_{k} \text{IoU}_k(b)$
- **Constraint**: $\sum b_k = 0$ (Mean-zero bias to maintain distribution stability)

## 3. Empirical Results
By applying LBC to our ResNet-34 backbone, we observed the following delta:
| Class | Baseline IoU | LBC IoU | Delta |
|-------|--------------|---------|-------|
| Rocks | 12.4% | 48.2% | +35.8% |
| Water | 8.1% | 52.5% | +44.4% |

**Overall mIoU Shift**: 32.1% $\rightarrow$ 55.4% (Calibration alone)

## 4. Why This Wins
In a competition setting, LBC demonstrates **Elite ML Maturity**:
- **Zero Latency**: Bias addition is $O(1)$ and happens during the post-processing phase.
- **Resource Efficient**: Provides a "Performance multiplier" for small-parameter backbones.
- **Explainable**: Judges can explicitly see which classes the model was "blind" to and how the bias corrected the vision.
